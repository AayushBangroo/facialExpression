{"cells":[{"metadata":{"_uuid":"4e5a4acc-deaa-47db-aedd-9126ed5e8657","_cell_guid":"a44744a1-1fb4-4ee6-ac40-78e607b4ee5f","trusted":true},"cell_type":"code","source":"import os\nimport csv\nimport torch\nimport torchvision.transforms as transforms\nfrom itertools import islice\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport cv2\nfrom torchvision.utils import make_grid\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"272939bb-11ec-4471-87df-f7f6de66cb08","_cell_guid":"52e4f5a4-9932-427d-8f01-df44ba9e1d69","trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/fer2013/fer2013.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc0daa68-106a-4287-b7a4-4d4d5a4d0547","_cell_guid":"f3089f1c-66c8-4723-8d4c-f21fc2e68f2f","trusted":true},"cell_type":"code","source":"data_df=pd.read_csv(DATA_DIR)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec137d6-8beb-4b95-83aa-12d4d1e5368f","_cell_guid":"2c788116-a974-4202-bc5f-957dd87d9c54","trusted":true},"cell_type":"code","source":"classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bb516f3-f491-45e4-9e35-ed398c2484e4","_cell_guid":"c1f52bb4-94f7-45aa-a861-92ec1f8dee8e","trusted":true},"cell_type":"code","source":"class DataSet(torch.utils.data.Dataset):\n\n    def __init__(self,transform=None, images=None, emotions=None):\n        self.transform = transform\n        self.images = images\n        self.emotions = emotions\n\n    def __getitem__(self, index):\n        image = self.images[index]\n        emotion = self.emotions[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, emotion\n\n    def __len__(self):\n        return len(self.images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b82701-579b-48b7-9c27-d333391b88ad","_cell_guid":"30f4b511-0000-416a-a297-09f290c259b4","trusted":true},"cell_type":"code","source":"class DataSetFactory:\n\n    def __init__(self):\n        images = []\n        emotions = []\n        val_images = []\n        val_emotions = []\n        test_images=[]\n        test_emotions=[]\n\n        with open('../input/fer2013/fer2013.csv', 'r') as csvin:\n            data = csv.reader(csvin)\n            next(data)\n            for row in data:\n                face = [int(pixel) for pixel in row[1].split()]\n                face = np.asarray(face).reshape(48, 48)\n                face = face.astype('uint8')\n\n                if row[-1] == 'Training':\n                    emotions.append(int(row[0]))\n                    images.append(Image.fromarray(face))\n                if row[-1] == \"PrivateTest\":\n                    val_emotions.append(int(row[0]))\n                    val_images.append(Image.fromarray(face))\n                if row[-1]==\"PublicTest\":\n                    test_emotions.append(int(row[0]))\n                    test_images.append(Image.fromarray(face))\n                \n\n        print('training size %d : val size %d : test_size %d'%(len(images), len(val_images), len(test_images)))\n        train_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ToTensor(),\n        ])\n        val_transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n\n        self.training = DataSet(transform=train_transform, images=images, emotions=emotions)\n        self.validation = DataSet(transform=val_transform, images=val_images, emotions=val_emotions)\n        self.testing = DataSet(transform=val_transform, images=test_images, emotions=test_emotions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be327305-0e5c-4c71-97a3-c0f489b84216","_cell_guid":"99cd5c80-05ce-46d8-8524-388315288bde","trusted":true},"cell_type":"code","source":"train_tfms = transforms.Compose([\n    transforms.RandomHorizontalFlip(), \n    transforms.RandomRotation(10),\n    transforms.ToTensor()\n])\n\nvalid_tfms = transforms.Compose([\n    transforms.ToTensor(), \n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc3408b6-6ab3-4c0e-87c3-43e83fee91d6","_cell_guid":"a3fa62d4-9a5a-4b41-a0be-ba8cbec64ae5","trusted":true},"cell_type":"code","source":"np.random.seed(42)\nmsk = np.random.rand(len(data_df)) < 0.9\n\ntrain_df = data_df[msk].reset_index()\nval_df = data_df[~msk].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ede5e0c-ec65-40c3-82e6-9ec0a7c6e588","_cell_guid":"00fcd5e4-7004-4009-b7bb-d906d7eb5520","trusted":true},"cell_type":"code","source":"batch_size=128\nfactory = DataSetFactory()\ntraining_loader = DataLoader(factory.training, batch_size=batch_size, shuffle=True, num_workers=2)\nvalidation_loader = DataLoader(factory.validation, batch_size=batch_size, shuffle=True, num_workers=2)\ntesting_loader=DataLoader(factory.testing, batch_size=64, shuffle=True, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"664e59cb-f2bf-4f64-9c23-648c495468e4","_cell_guid":"e0ced038-07c1-48c1-acf9-d540bdda590e","trusted":true},"cell_type":"code","source":"def decode_target(target, text_labels=False, threshold=0.5):\n    result = []\n    if text_labels:\n        result.append(classes[target] + \"(\" + str(target) + \")\")\n        return ' '.join(result)\n    \n    else:\n        for i, x in enumerate(target):\n            if (x == torch.max(target)):\n                result.append(classes[i] + \"(\" + str(i) + \")\")\n        return ' '.join(result)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5460a893-3f70-4dce-8b8a-dfd3b1759514","_cell_guid":"89fdc4ed-80ca-4267-9f10-4107d24e92a2","trusted":true},"cell_type":"code","source":"def show_sample(img, target):\n    print(img)\n    img=img.squeeze(0)\n    print(img.shape)\n    plt.imshow(img)\n    print('Labels:',decode_target(target,text_labels=True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcba3181-0251-4adf-b327-9e42205fae93","_cell_guid":"ddf66a09-7960-43dc-b37a-31c5846b4f43","trusted":true},"cell_type":"code","source":"def show_predicted(img, target):\n    print(img)\n    img=img.squeeze(0)\n    print(img.shape)\n    plt.imshow(img)\n    print('Labels:',decode_target(target))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ab647ee-7d29-4687-af92-6f56fb479965","_cell_guid":"31b6c5c7-634d-4934-b2b1-35e7d6d52f31","trusted":true},"cell_type":"code","source":"show_sample(*factory.training[7])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a8b814a-3155-4aa1-bf41-6379399fb318","_cell_guid":"b51eff73-583c-4ab4-ab9f-ee3b90961361","trusted":true},"cell_type":"code","source":"for images, _ in training_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9506d9f-84cd-465e-b17a-d3d4a04e58b2","_cell_guid":"8958252a-2841-4ab7-8013-0af2237359cc","trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176b84dc-a3a5-4206-bfcb-e4b106dd17ea","_cell_guid":"b3831708-949c-44ad-84ba-bb1bde9ea1b7","trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07bc6ff0-c8e4-453a-862d-b99895b533e1","_cell_guid":"069c3214-eb71-43e4-8db5-33b5699313c6","trusted":true},"cell_type":"code","source":"device = get_default_device()\ndevice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26514446-1c3b-4aa4-9c48-30adaa3af483","_cell_guid":"7f14f687-786d-4f58-83f4-056955733f46","trusted":true},"cell_type":"code","source":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e56a8d3b-c8bd-4416-ac5c-7bd521d38286","_cell_guid":"52c615f2-0b4a-4d69-8cbb-e1460492776c","trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9aaeb76-7f03-4633-9247-1812aa36b7f3","_cell_guid":"206fee1a-2224-4e9e-b2c0-774690b3b0c9","trusted":true},"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c223e86d-b21f-43f7-aafc-6b86d358c1d6","_cell_guid":"9a31ec7b-b0fd-42cd-81c0-c546932906eb","trusted":true},"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9e9b2e-5c40-49f7-94d2-4e9ce3687385","_cell_guid":"a36fc773-f917-48f1-99e3-c438db843a98","trusted":true},"cell_type":"code","source":"class Face(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],last_lr: {:.4f},train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch,result['lrs'][-1],result['train_loss'],result['val_loss'], result['val_acc']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dee1ad6a-be99-49ca-9cc9-81fec1ac22e2","_cell_guid":"73995f3f-72cb-46e9-9fda-6e3d39c235cf","trusted":true},"cell_type":"code","source":"training_loader = DeviceDataLoader(training_loader, device)\nvalidation_loader = DeviceDataLoader(validation_loader, device)\ntesting_loader = DeviceDataLoader(testing_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adb1c506-4d72-42b0-83aa-b752103e540e","_cell_guid":"705e2192-aa58-4467-babb-18aedcef789e","trusted":true},"cell_type":"code","source":"class SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels,\n                                   bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, in_channeld, out_channels):\n        super(ResidualBlock, self).__init__()\n\n        self.residual_conv = nn.Conv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=1, stride=2,\n                                       bias=False)\n        self.residual_bn = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n\n        self.sepConv1 = SeparableConv2d(in_channels=in_channeld, out_channels=out_channels, kernel_size=3, bias=False,\n                                        padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n        self.relu = nn.ReLU()\n\n        self.sepConv2 = SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False,\n                                        padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n        self.maxp = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        res = self.residual_conv(x)\n        res = self.residual_bn(res)\n        x = self.sepConv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.sepConv2(x)\n        x = self.bn2(x)\n        x = self.maxp(x)\n        return res + x\n\n\nclass FaceCnnModel(Face):\n\n    def __init__(self):\n        super(FaceCnnModel, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(8, affine=True, momentum=0.99, eps=1e-3)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(8, momentum=0.99, eps=1e-3)\n        self.relu2 = nn.ReLU()\n\n        self.module1 = ResidualBlock(in_channeld=8, out_channels=16)\n        self.module2 = ResidualBlock(in_channeld=16, out_channels=32)\n        self.module3 = ResidualBlock(in_channeld=32, out_channels=64)\n        self.module4 = ResidualBlock(in_channeld=64, out_channels=128)\n\n        self.last_conv = nn.Conv2d(in_channels=128, out_channels=7, kernel_size=3, padding=1)\n        self.avgp = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward(self, input):\n        x = input\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.module1(x)\n        x = self.module2(x)\n        x = self.module3(x)\n        x = self.module4(x)\n        x = self.last_conv(x)\n        x = self.avgp(x)\n        x = x.view((x.shape[0], -1))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31c26903-5475-4b67-91fb-9b976e4e1150","_cell_guid":"b6d7717b-28df-4bfe-9286-c1a3ece7a0f0","trusted":true},"cell_type":"markdown","source":"# **Training**","execution_count":null},{"metadata":{"_uuid":"8f1334f6-4acf-41e0-91b9-c2861bb8d602","_cell_guid":"0201c5cb-c636-45be-b91d-06c63d679770","trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2faf9c8b-0fa1-4ba1-982f-2231e8a46f7b","_cell_guid":"7c3a3216-3e0d-4a83-9f61-f2f874785a59","trusted":true},"cell_type":"code","source":"model = to_device(FaceCnnModel(), device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e560f6a-b605-4585-bfd3-18c935546066","_cell_guid":"ca4fd282-f84d-481f-8051-bd291388352a","trusted":true},"cell_type":"code","source":"history = [evaluate(model, validation_loader)]\nhistory","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee17bba6-02b1-4dbc-a623-7ddfe8cdbcac","_cell_guid":"65e19dc6-c91e-4f27-a74a-c800ba442c72","trusted":true},"cell_type":"code","source":"max_lr=0.009\ngrad_clip = 0.1\nweight_decay = 1e-4\nepochs=20\nopt_func = torch.optim.Adam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7c9a178-46e7-4201-b087-4753f545a049","_cell_guid":"50323edb-fde6-4c12-aea7-94c81f54eb3e","trusted":true},"cell_type":"code","source":"history += fit_one_cycle(epochs, max_lr, model, training_loader, validation_loader, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"380e59fe-aee8-44a2-b21f-062dacf9963c","_cell_guid":"86dd03e7-6d56-4400-81de-ef0915f009f7","trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6968d815-1969-4d37-860e-f5990b1d33f4","_cell_guid":"f5191483-fee9-4b83-9ea7-e855daabdf94","trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02685b50-7dcd-46b8-a18e-3202995a4239","_cell_guid":"ccc0e48c-3268-432f-b1b4-a5384faa68a7","trusted":true},"cell_type":"code","source":"plot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e82fee97-bda7-4273-8a61-09abdf5b6c43","_cell_guid":"0952d36d-ca34-4e9e-b3ce-73a9e616b659","trusted":true},"cell_type":"code","source":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9241300-3662-4b28-a200-adc0466f04d5","_cell_guid":"c36709c5-8506-4541-8693-2bd95ea5e06a","trusted":true},"cell_type":"code","source":"plot_lrs(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"408101fd-7644-42c6-b961-b11383c0df6f","_cell_guid":"5e5031f2-18c8-4f6a-acbe-127fc20e000c","trusted":true},"cell_type":"code","source":"def predict_single(image):\n    xb = image.unsqueeze(0)\n    xb = to_device(xb, device)\n    preds = model(xb)\n    prediction = preds[0]\n    index = prediction.cpu().data.numpy().argmax()\n    print(\"Prediction: \", prediction)\n    show_predicted(image, prediction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e4d9f45-f913-404a-ab63-0a14705ee709","_cell_guid":"93c1376d-9095-4694-9d49-975f04193643","trusted":true},"cell_type":"code","source":"predict_single(factory.testing[40][0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c50a82-2d27-43bb-8645-229cfe9d5a7e","_cell_guid":"9b300142-3355-44f9-880c-01535ff831ff","trusted":true},"cell_type":"code","source":"\ntorch.save(model.state_dict(), 'face_cnn.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}